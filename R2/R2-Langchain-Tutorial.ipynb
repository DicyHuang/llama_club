{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 是圍繞 LLMs 構建的框架。我們可以將其用於聊天機器人、Question-Answering (QA)、摘要等等。\n",
    "\n",
    "這個函式庫的核心思想是我們可以將不同的元件 “鏈結” 在一起，以創建更多元的 LLMs 應用。 Chain 來自幾個 Module 的多個組件：\n",
    "\n",
    "1. **Prompt templates**：Prompt templates 是不同類型提示的範本。例如「 chatbot 」樣式模板、ELI5 問答等\n",
    "2. **LLMs**：像 GPT-3、Mistral、Llama、Breeze、TAIDE 等大型語言模型\n",
    "3. **Agents**：Agents 使用 LLMs 決定應採取的動作。可以使用網路搜尋(Google Search)或計算器(Python func)之類的工具，並將所有工具包裝成一個邏輯循環的操作。\n",
    "4. **Memory**：短期記憶、長期記憶。\n",
    "\n",
    "我們將從 Prompt templates 和 LLMs 的基礎知識開始。以下教學將提供兩個 LLMs 選項，包含 Hugging Face Hub 或 Hugging Face Pipeline 的模型。\n",
    "\n",
    "- GitHub: https://github.com/langchain-ai/langchain\n",
    "- Docs: https://python.langchain.com/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLMs\n",
    "若要使用 Huggingface 上的 Model 有兩種方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Huggingface Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config)\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        generation_config=generation_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "提問: 2023 年全球最賣座的電影是哪一部?\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "解答: \"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型初始化\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id=MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        'temperature': 1e-10,\n",
    "        'max_new_tokens': 1024,\n",
    "        'top_p': 0.95,\n",
    "        'repetition_penalty': 1.15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "提問: NBA 2023 年總冠軍球隊是誰?\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "解答: \"\"\"\n",
    "hub_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt\n",
    "一個好的 Prompt 通常包含以下四個組成部分：\n",
    "\n",
    "1. **指示**: 告訴模型要做什麼，如何使用提供的信息，如何處理查詢，並建立輸出\n",
    "2. **範例輸入**: 提供範例輸入，以向模型示範預期的內容\n",
    "3. **範例輸出**: 提供對應的範例輸出\n",
    "4. **查詢**: 您希望模型處理的實際輸入\n",
    "\n",
    "以下介紹幾種在 LangChain 上使用 Pormpt 的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"告訴我一個笑話\")\n",
    "prompt_template.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"告訴我關於一個{content}的{adjective}笑話。\")\n",
    "prompt_template.format(adjective=\"悲傷的\", content=\"數據科學家\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一個擁有強大能力的 AI 機器人。你的名字是{name}。\"),\n",
    "        (\"human\", \"你好，你好嗎？\"),\n",
    "        (\"ai\", \"我很好，謝謝！\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"你叫什麼名字？\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 混合使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"你是一個擁有強大能力的 AI 機器人。\"\n",
    "question_prompt = \"告訴我關於一個{content}的{adjective}笑話。\"\n",
    "\n",
    "full_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(\"你好，你好嗎？\"),\n",
    "    AIMessage(\"我很好，謝謝！\"),\n",
    "    HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=question_prompt,\n",
    "            input_variables=[\"content\", \"adjective\"])\n",
    "    )\n",
    "])\n",
    "full_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Chain\n",
    "對於簡單的任務，使用單一 LLM（大型語言模型）效果很好。然而，對於更複雜的任務，通常需要鍊式多個步驟和/或模型。\n",
    "\n",
    "在LangChain中，可以使用傳統的 LLMChain，較新且建議的方法是 LangChain 表達式語言（LCEL）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"問題: {question}\n",
    "\n",
    "Let's think step by step.\n",
    "\n",
    "答案: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# 使用 LLM Chain 將 Prompt 與 LLM 串接起來\n",
    "llm_chain = LLMChain(\n",
    "    prompt=\"提問: {question}\",\n",
    "    llm=llm)\n",
    "\n",
    "# 將問題透過參數化的方式帶入\n",
    "question = \"NBA 2023 年總冠軍球隊是誰?\"\n",
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LCEL（LangChain Expression Language）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"你是一位專業的資料科學家和機器學習工程師，能夠提供專業的知識並準確地回答問題。\",),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 使用 LCEL 將 Prompt 與 LLM 等串接起來\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 將問題透過參數化的方式帶入\n",
    "llm_chain.invoke({\"question\": \"機器學習和深度學習有什麼不同？\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
